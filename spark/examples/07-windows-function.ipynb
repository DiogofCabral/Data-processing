{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/07-windows-function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz2SIPu5OKEt"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/07-windows-function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Windows Function\n",
        "- Window functions operate on a group of rows, referred to as a window, and calculate a return value for each row based on the group of rows.\n",
        "- Window functions are useful for processing tasks such as calculating a moving average, computing a cumulative statistic, or accessing the value of rows given the relative position of the current row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "c410e46c-4a50-43aa-926f-d0417c6280d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BRgjNQbOKE0"
      },
      "source": [
        "# Windows Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE TABLE employees (name STRING, dept STRING, salary INT, age INT);\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BvB3TyVROQ5_",
        "outputId": "19f22233-4452-416c-8351-5061b869eeac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`employees`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b43b87704dad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE TABLE employees (name STRING, dept STRING, salary INT, age INT);\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`employees`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fZzWrJbROKE0",
        "outputId": "54903537-591c-4ab2-dd54-45a4e835cd55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+------+---+\n",
            "| name|       dept|salary|age|\n",
            "+-----+-----------+------+---+\n",
            "| Lisa|      Sales| 10000| 35|\n",
            "| Evan|      Sales| 32000| 38|\n",
            "| Fred|Engineering| 21000| 28|\n",
            "| Alex|      Sales| 30000| 33|\n",
            "|  Tom|Engineering| 23000| 33|\n",
            "| Jane|  Marketing| 29000| 28|\n",
            "| Jeff|  Marketing| 35000| 38|\n",
            "| Paul|Engineering| 29000| 23|\n",
            "|Chloe|Engineering| 23000| 25|\n",
            "+-----+-----------+------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# using inline table to prepare the data\n",
        "\n",
        "qry = \"\"\"CREATE OR REPLACE TEMPORARY VIEW employees AS SELECT * FROM VALUES(\"Lisa\", \"Sales\", 10000, 35),\n",
        "(\"Evan\", \"Sales\", 32000, 38),\n",
        "(\"Fred\", \"Engineering\", 21000, 28),\n",
        "(\"Alex\", \"Sales\", 30000, 33),\n",
        "(\"Tom\", \"Engineering\", 23000, 33),\n",
        "(\"Jane\", \"Marketing\", 29000, 28),\n",
        "(\"Jeff\", \"Marketing\", 35000, 38),\n",
        "(\"Paul\", \"Engineering\", 29000, 23),\n",
        "(\"Chloe\", \"Engineering\", 23000, 25)\n",
        "AS employees(name, dept, salary, age)\"\"\"\n",
        "\n",
        "spark.sql(qry)\n",
        "spark.table(\"employees\").show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate dense_rank on salary\n",
        "qry1 = \"\"\"SELECT name, dept, salary, DENSE_RANK() OVER (PARTITION BY dept ORDER BY salary ROWS BETWEEN\n",
        "    UNBOUNDED PRECEDING AND CURRENT ROW) AS dense_rank FROM employees;\"\"\"\n",
        "\n",
        "spark.sql(qry1).show()"
      ],
      "metadata": {
        "id": "o0JtIU70Pgoq",
        "outputId": "30d0d190-efc7-437b-af2b-88f6b5b1eb78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+------+----------+\n",
            "| name|       dept|salary|dense_rank|\n",
            "+-----+-----------+------+----------+\n",
            "| Fred|Engineering| 21000|         1|\n",
            "|  Tom|Engineering| 23000|         2|\n",
            "|Chloe|Engineering| 23000|         2|\n",
            "| Paul|Engineering| 29000|         3|\n",
            "| Jane|  Marketing| 29000|         1|\n",
            "| Jeff|  Marketing| 35000|         2|\n",
            "| Lisa|      Sales| 10000|         1|\n",
            "| Alex|      Sales| 30000|         2|\n",
            "| Evan|      Sales| 32000|         3|\n",
            "+-----+-----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate min salary by dept\n",
        "qry2 = \"\"\"SELECT name, dept, salary, MIN(salary) OVER (PARTITION BY dept ORDER BY salary) AS min\n",
        "    FROM employees;\"\"\"\n",
        "\n",
        "spark.sql(qry2).show()"
      ],
      "metadata": {
        "id": "mE58WtSzRsg9",
        "outputId": "9600d62c-9317-40a5-d196-b3ccc763e40f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+------+-----+\n",
            "| name|       dept|salary|  min|\n",
            "+-----+-----------+------+-----+\n",
            "| Fred|Engineering| 21000|21000|\n",
            "|  Tom|Engineering| 23000|21000|\n",
            "|Chloe|Engineering| 23000|21000|\n",
            "| Paul|Engineering| 29000|21000|\n",
            "| Jane|  Marketing| 29000|29000|\n",
            "| Jeff|  Marketing| 35000|29000|\n",
            "| Lisa|      Sales| 10000|10000|\n",
            "| Alex|      Sales| 30000|10000|\n",
            "| Evan|      Sales| 32000|10000|\n",
            "+-----+-----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same logic but using dataframes\n",
        "from pyspark.sql.functions import min\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df = spark.table(\"employees\")\n",
        "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
        "df.withColumn(\"min\", min(\"salary\").over(windowSpec)).show()"
      ],
      "metadata": {
        "id": "FBohSBV4Sfkb",
        "outputId": "8e9a0255-f22b-4c0d-d0dc-3f4d5d00cbec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+------+---+-----+\n",
            "| name|       dept|salary|age|  min|\n",
            "+-----+-----------+------+---+-----+\n",
            "| Fred|Engineering| 21000| 28|21000|\n",
            "|  Tom|Engineering| 23000| 33|21000|\n",
            "|Chloe|Engineering| 23000| 25|21000|\n",
            "| Paul|Engineering| 29000| 23|21000|\n",
            "| Jane|  Marketing| 29000| 28|29000|\n",
            "| Jeff|  Marketing| 35000| 38|29000|\n",
            "| Lisa|      Sales| 10000| 35|10000|\n",
            "| Alex|      Sales| 30000| 33|10000|\n",
            "| Evan|      Sales| 32000| 38|10000|\n",
            "+-----+-----------+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question"
      ],
      "metadata": {
        "id": "8KnCXs19UXDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "# Use windows functions to identify the higher salary by dept\n",
        "# Create new column \"highest_salary_dept\" and assign TRUE/FALSE accordingly\n",
        "# Identify the high salary of the company (including all the dept)\n",
        "# Create new column \"highest_salary_company\" and assign TRUE/FALSE accordingly"
      ],
      "metadata": {
        "id": "NM33rISwSybn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}