{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/rdds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5heaIQ2sMZ24"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/rdds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Data Collections\n",
        "- RDDs\n",
        "- ~~Datasets ~~ (only Java and Scala=)\n",
        "- DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "26e9c739-372b-4a72-e685-785fe4ada605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkSession.getActiveSession().sparkContext"
      ],
      "metadata": {
        "id": "SqwZjdL9MnL6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGG7R76hMZ3E"
      },
      "source": [
        "# RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DZaxZW0bMZ3F"
      },
      "outputs": [],
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "distData = sc.parallelize(data)\n",
        "\n",
        "### Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program\n",
        "### The elements of the collection are copied to form a distributed dataset that can be operated on in parallel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a8eCChgrMZ3H"
      },
      "outputs": [],
      "source": [
        "### Run parallel operations\n",
        "distData.reduce(lambda a, b: a + b)\n",
        "\n",
        "### One important parameter for parallel collections is the number of partitions to cut the dataset into\n",
        "### Spark will run one task for each partition of the cluster\n",
        "### Typically you want 2-4 partitions for each CPU in your cluster\n",
        "### Normally, Spark tries to set the number of partitions automatically based on your cluster\n",
        "### However, you can also set it manually\n",
        "\n",
        "distData = sc.parallelize(data, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qs9KNmJMZ3I"
      },
      "outputs": [],
      "source": [
        "### Read from external datasets\n",
        "distFile = sc.textFile(\"data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1ftjIsEMZ3J"
      },
      "outputs": [],
      "source": [
        "### RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset\n",
        "\n",
        "### Example:\n",
        "### map is a transformation that passes each dataset element through a function and returns a new RDD representing the results\n",
        "### reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJr1dWvFMZ3J"
      },
      "outputs": [],
      "source": [
        "### All transformations in Spark are lazy\n",
        "### The transformations are only computed when an action requires a result to be returned to the driver program\n",
        "\n",
        "lines = sc.textFile(\"data.txt\")\n",
        "lineLengths = lines.map(lambda s: len(s))\n",
        "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
        "\n",
        "### The first line defines a base RDD from an external file.\n",
        "### lines is merely a pointer to the file\n",
        "### The second line defines lineLengths as the result of a map transformation\n",
        "### lineLengths is not immediately computed, due to laziness\n",
        "### reducer is an actions so Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.\n",
        "\n",
        "### if we want to use lineLenghts again\n",
        "lineLengths.persist()\n",
        "### before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmHNfHpGMZ3L"
      },
      "source": [
        "# DataFrames"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}